# # -*- coding: utf-8 -*-
# """Prueba 1 de red

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1ifVzYoi7iw1Y1-XHsPwTJHeOCo8vrfsr
# """

# import keras
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.preprocessing import LabelEncoder
# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout
# from tensorflow.keras.utils import to_categorical

# # Usamos panda para cargar el dataset para estudiarlo
# data = pd.read_csv('dataset2.csv', sep=';')
# data.head()

# data.info()

# print(data.columns)

# # Separar las características (síntomas) y el objetivo (enfermedades)
# #X =data.drop(columns = ['diseases'], axis=1)
# # Copiar la columna a una variable
# y = data['diseases'].copy()

# # Eliminar la columna del DataFrame
# #X=data.drop('diseases', axis=1, inplace=True)
# X=data.drop(columns=['diseases'], axis=1)  # Características (síntomas)
# #X = data.drop(columns='diseases') # Características (síntomas), esto borra el diseases dejando solamente los sintomas

# y

# print(data.columns.tolist())  # Muestra los nombres de todas las columnas como una lista

# # Verificar los valores únicos en la columna 'diseases'
# print(data['diseases'].unique())

# print("-----------------------------YOU RIGHT HERE---------------------------------------")

# # Ahora, convertir la columna de enfermedades a valores numéricos
# le = LabelEncoder()
# y = le.fit_transform(data['diseases'])
# y = to_categorical(y)  # Convertir las variables objetivo en formato categórico

# print(y)

# #le = LabelEncoder()
# #Convertir los datos escritos de y como las enfermedades, en valores numericos como por ejemplo 1,2,3,4 para el caso de 4 enfermedades distintas
# #y = le.fit_transform(data['diseases'])
# #y = to_categorical(y)  # Convertir las variables objetivas (las enfermedades a datos para su clasificacion)

# # Dividir los datos en conjuntos de entrenamiento y prueba
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# # Escalar las características
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)
# print(X_train)
# print("-------------------------------------------")
# print(X_test)

# # Construir la red neuronal
# model = Sequential()

# # Creacion de las capas ocultas :D
# #1
# model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
# model.add(Dropout(0.20))  # Regularización

# #2
# model.add(Dense(64, activation='relu'))
# model.add(Dropout(0.20))  # Regularización

# #3
# model.add(Dense(128, activation='relu'))
# model.add(Dropout(0.20))  # Regularización

# #4
# model.add(Dense(64, activation='relu'))
# model.add(Dropout(0.20))  # Regularización

# #5
# model.add(Dense(64, activation='relu'))
# model.add(Dropout(0.20))  # Regularización

# model.add(Dense(y.shape[1], activation='softmax'))  # Capa de salida para múltiples clases

# # Compilar el modelo
# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# #el loss de categorical_croseentropy nos sive porque nuestro problema es de clasificacion en vez de regresion como habia dicho xd
# #entonces al no ser de regresion el mse o mean_square_error ya no es necesario y solamente obstruiria por ahora ando terminando de comprender correctamente todo para
# #para poder graficarlo y tener un mejor entendimiento de la red.

# # Entrenar el modelo
# print("Comenzando a entrenamiento....")
# historial = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.2)
# # Pusimos el batch size para que, joa mejor lo pongo en otro cuadro 
# print("Modelo entrenado ;3 ponlo aprueba")

# """El batch size es de 128 porque actualizara 120 veces los pesos de manera simultanea, hize una simulacion con chatgpt y para mi equipo es un punto intermedio, aunque como estamos en google colab depronto podramos aumentar el tamaño del batch ?, no se que opinen,

# Por sio acaso
# epoca (epochs): Una pasada completa por todo el dataset.

# Batch: La cantidad de datos que el modelo procesa antes de actualizar los pesos.

# Encontre la cosa esa que grafica la funcion de la red :DDDD
# """

# import matplotlib.pyplot as plt
# plt.xlabel("# de epoca (son los intentos que hizo)")
# plt.ylabel("margen de perdida o error")
# plt.plot(historial.history["loss"])

# plt.plot(historial.history['accuracy'], label='Train Accuracy')
# plt.plot(historial.history['val_accuracy'], label='Validation Accuracy')
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')

# # Evaluar el modelo
# loss, accuracy = model.evaluate(X_test, y_test)
# print(f'Loss: {loss}, Accuracy: {accuracy}')

# from math import e

# data = pd.read_csv('dataset2.csv', sep=';')


# # Lista de todos los síntomas, usando el drop data para tomar los necesarios
# sintomas = data.drop(columns=['diseases'], axis=1).columns.tolist()
# print(sintomas)
# print("------------------------1---------------------------")
# #Lista de enfermedades
# enfermedades = data['diseases'].unique().tolist()
# print(enfermedades)
# print("------------------------2---------------------------")

# #Aca tomamos los sintomas y verificamos
# sintomas_paciente = ["fever","diarrhea","vomit","lower abdominal pain"]

# # Crear un vector de entrada con 0's del tamaño de `all_symptoms`
# vector_entrada_dato = [1 if dato in sintomas_paciente else 0 for dato in sintomas]

# # Asegúrate de que sea un array de numpy y tenga la forma correcta para la red neuronal
# vector_entrada_dato = np.array(vector_entrada_dato).reshape(1, -1)

# # Hacer la predicción
# result = model.predict(vector_entrada_dato)
# predicted_class = np.argmax(result)  # Si necesitas la clase predicha


# print(f"----->: {result}")
# print(f"Su enfermedad es: {enfermedades[predicted_class]}")

# model.save('botci.h5')



"""
✨Aquí yace Botci — una red neuronal pequeña, pero valiente.
Soñó con predecir, lo intentó con todas sus capas y pesos,
pero el mundo era caótico y los datos, traicioneros.
Hoy le decimos adiós,
no como un fracaso,
sino como un paso en el aprendizaje.
Gracias por las horas, los intentos, los ajustes de tasa de aprendizaje.
Descansa en paz en el historial de experimentos.
Tu código quedará en nuestra memoria (y en GitHub). ✨

✨✨

"""




