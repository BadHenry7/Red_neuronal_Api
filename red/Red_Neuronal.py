# # -*- coding: utf-8 -*-
# """Prueba 1 de red

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1ifVzYoi7iw1Y1-XHsPwTJHeOCo8vrfsr
# """

import keras
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Usamos panda para cargar el dataset para estudiarlo
data = pd.read_csv('dataset2.csv', sep=';')
data.head()

data.info()

print(data.columns)

# Separar las características (síntomas) y el objetivo (enfermedades)
#X =data.drop(columns = ['diseases'], axis=1)
# Copiar la columna a una variable
y = data['diseases'].copy()

# Eliminar la columna del DataFrame
#X=data.drop('diseases', axis=1, inplace=True)
X=data.drop(columns=['diseases'], axis=1)  # Características (síntomas)
#X = data.drop(columns='diseases') # Características (síntomas), esto borra el diseases dejando solamente los sintomas

y

print(data.columns.tolist())  # Muestra los nombres de todas las columnas como una lista

# Verificar los valores únicos en la columna 'diseases'
print(data['diseases'].unique())

print("-----------------------------YOU RIGHT HERE---------------------------------------")

# Ahora, convertir la columna de enfermedades a valores numéricos
le = LabelEncoder()
y = le.fit_transform(data['diseases'])
y = to_categorical(y)  # Convertir las variables objetivo en formato categórico

print(y)

#le = LabelEncoder()
#Convertir los datos escritos de y como las enfermedades, en valores numericos como por ejemplo 1,2,3,4 para el caso de 4 enfermedades distintas
#y = le.fit_transform(data['diseases'])
#y = to_categorical(y)  # Convertir las variables objetivas (las enfermedades a datos para su clasificacion)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Escalar las características
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
print(X_train)
print("-------------------------------------------")
print(X_test)

# Construir la red neuronal
model = Sequential()

# Creacion de las capas ocultas :D
#1
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.20))  # Regularización

#2
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.20))  # Regularización

#3
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.20))  # Regularización

#4
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.20))  # Regularización

#5
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.20))  # Regularización

model.add(Dense(y.shape[1], activation='softmax'))  # Capa de salida para múltiples clases

# Compilar el modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
#el loss de categorical_croseentropy nos sive porque nuestro problema es de clasificacion en vez de regresion como habia dicho xd
#entonces al no ser de regresion el mse o mean_square_error ya no es necesario y solamente obstruiria por ahora ando terminando de comprender correctamente todo para
#para poder graficarlo y tener un mejor entendimiento de la red.

# Entrenar el modelo
print("Comenzando a entrenamiento....")
historial = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.2)
# Pusimos el batch size para que, joa mejor lo pongo en otro cuadro 
print("Modelo entrenado ;3 ponlo aprueba")

"""El batch size es de 128 porque actualizara 120 veces los pesos de manera simultanea, hize una simulacion con chatgpt y para mi equipo es un punto intermedio, aunque como estamos en google colab depronto podramos aumentar el tamaño del batch ?, no se que opinen,

Por sio acaso
epoca (epochs): Una pasada completa por todo el dataset.

Batch: La cantidad de datos que el modelo procesa antes de actualizar los pesos.

Encontre la cosa esa que grafica la funcion de la red :DDDD
"""

import matplotlib.pyplot as plt
plt.xlabel("# de epoca (son los intentos que hizo)")
plt.ylabel("margen de perdida o error")
plt.plot(historial.history["loss"])

plt.plot(historial.history['accuracy'], label='Train Accuracy')
plt.plot(historial.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

# Evaluar el modelo
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

from math import e

data = pd.read_csv('dataset2.csv', sep=';')


# Lista de todos los síntomas, usando el drop data para tomar los necesarios
sintomas = data.drop(columns=['diseases'], axis=1).columns.tolist()
print(sintomas)
print("------------------------1---------------------------")
#Lista de enfermedades
enfermedades = data['diseases'].unique().tolist()
print(enfermedades)
print("------------------------2---------------------------")

#Aca tomamos los sintomas y verificamos
sintomas_paciente = ["fever","diarrhea","vomit","lower abdominal pain"]

# Crear un vector de entrada con 0's del tamaño de `all_symptoms`
vector_entrada_dato = [1 if dato in sintomas_paciente else 0 for dato in sintomas]

# Asegúrate de que sea un array de numpy y tenga la forma correcta para la red neuronal
vector_entrada_dato = np.array(vector_entrada_dato).reshape(1, -1)

# Hacer la predicción
result = model.predict(vector_entrada_dato)
predicted_class = np.argmax(result)  # Si necesitas la clase predicha


print(f"----->: {result}")
print(f"Su enfermedad es: {enfermedades[predicted_class]}")

model.save('botci.h5')



"""
✨Aquí yace Botci — una red neuronal pequeña, pero valiente.
Soñó con predecir, lo intentó con todas sus capas y pesos,
pero el mundo era caótico y los datos, traicioneros.
Hoy le decimos adiós,
no como un fracaso,
sino como un paso en el aprendizaje.
Gracias por las horas, los intentos, los ajustes de tasa de aprendizaje.
Descansa en paz en el historial de experimentos.
Tu código quedará en nuestra memoria (y en GitHub). ✨

✨✨

"""




